{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import k_armed_bandit as lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "q_real = [0, 4, 2, 9, 1, 3, 8, 6, 7, 5]\n",
    "iteration_no = 10000\n",
    "epsilon = 0.1\n",
    "standard_deviation = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon greedy action value method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of taking best posible action in 10000 itearation: 90.34 %, additional_factor:  0.10\n",
      "\n",
      "Average reward:     8.53\n",
      "\n",
      "Q[0] =  0.07\tq_real[0] =  0.00\tN[0] = 114.00\n",
      "Q[1] =  3.96\tq_real[1] =  4.00\tN[1] = 104.00\n",
      "Q[2] =  2.00\tq_real[2] =  2.00\tN[2] = 108.00\n",
      "Q[3] =  9.00\tq_real[3] =  9.00\tN[3] = 9034.00\n",
      "Q[4] =  1.12\tq_real[4] =  1.00\tN[4] = 91.00\n",
      "Q[5] =  3.06\tq_real[5] =  3.00\tN[5] = 98.00\n",
      "Q[6] =  8.07\tq_real[6] =  8.00\tN[6] = 104.00\n",
      "Q[7] =  5.98\tq_real[7] =  6.00\tN[7] = 135.00\n",
      "Q[8] =  7.05\tq_real[8] =  7.00\tN[8] = 89.00\n",
      "Q[9] =  5.01\tq_real[9] =  5.00\tN[9] = 123.00\n"
     ]
    }
   ],
   "source": [
    "Q = [0] * k\n",
    "N = [0] * k\n",
    "cumulative_reward = 0   # just for statistics purpose\n",
    "for i in range(iteration_no):\n",
    "    probability = np.random.rand()\n",
    "    if probability > epsilon:\n",
    "        action = lib.get_greedy_action(Q)\n",
    "    else:\n",
    "        action = np.random.randint(k)\n",
    "\n",
    "    reward = lib.get_action_reward(q_real[action], standard_deviation)\n",
    "    N[action] += 1\n",
    "    Q[action] += (reward - Q[action]) / N[action]\n",
    "    cumulative_reward += reward\n",
    "lib.print_stats(\n",
    "    q_real=q_real,\n",
    "    N=N,\n",
    "    iteration_no=iteration_no,\n",
    "    cumulative_reward=cumulative_reward,\n",
    "    Q=Q,\n",
    "    epsilon=epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of taking best posible action in 10000 itearation: 99.91 %, additional_factor: 10.00\n",
      "\n",
      "Average reward:     8.99\n",
      "\n",
      "Q[0] = -0.39\tq_real[0] =  0.00\tN[0] =  1.00\n",
      "Q[1] =  4.06\tq_real[1] =  4.00\tN[1] =  1.00\n",
      "Q[2] = -0.80\tq_real[2] =  2.00\tN[2] =  1.00\n",
      "Q[3] =  8.99\tq_real[3] =  9.00\tN[3] = 9991.00\n",
      "Q[4] =  2.66\tq_real[4] =  1.00\tN[4] =  1.00\n",
      "Q[5] =  3.17\tq_real[5] =  3.00\tN[5] =  1.00\n",
      "Q[6] =  8.91\tq_real[6] =  8.00\tN[6] =  1.00\n",
      "Q[7] =  5.23\tq_real[7] =  6.00\tN[7] =  1.00\n",
      "Q[8] =  6.18\tq_real[8] =  7.00\tN[8] =  1.00\n",
      "Q[9] =  5.87\tq_real[9] =  5.00\tN[9] =  1.00\n"
     ]
    }
   ],
   "source": [
    "init_value = 10\n",
    "Q = [init_value] * k\n",
    "N = [0] * k\n",
    "cumulative_reward = 0   # just for statistics purpose\n",
    "for i in range(iteration_no):\n",
    "    action = lib.get_greedy_action(Q)\n",
    "    reward = lib.get_action_reward(q_real[action], standard_deviation)\n",
    "    N[action] += 1\n",
    "    Q[action] += (reward - Q[action]) / N[action]\n",
    "    cumulative_reward += reward\n",
    "lib.print_stats(\n",
    "    q_real=q_real,\n",
    "    N=N,\n",
    "    iteration_no=iteration_no,\n",
    "    cumulative_reward=cumulative_reward,\n",
    "    Q=Q,\n",
    "    init_value=init_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Upper-Confidence-Bound Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of taking best posible action in 10000 itearation: 99.81 %, additional_factor:  1.00\n",
      "\n",
      "Average reward:     8.98\n",
      "\n",
      "Q[0] = -1.15\tq_real[0] =  0.00\tN[0] =  1.00\n",
      "Q[1] =  3.55\tq_real[1] =  4.00\tN[1] =  1.00\n",
      "Q[2] =  2.94\tq_real[2] =  2.00\tN[2] =  1.00\n",
      "Q[3] =  8.99\tq_real[3] =  9.00\tN[3] = 9981.00\n",
      "Q[4] = -0.08\tq_real[4] =  1.00\tN[4] =  1.00\n",
      "Q[5] =  4.08\tq_real[5] =  3.00\tN[5] =  1.00\n",
      "Q[6] =  7.79\tq_real[6] =  8.00\tN[6] =  7.00\n",
      "Q[7] =  6.43\tq_real[7] =  6.00\tN[7] =  2.00\n",
      "Q[8] =  6.54\tq_real[8] =  7.00\tN[8] =  3.00\n",
      "Q[9] =  5.64\tq_real[9] =  5.00\tN[9] =  2.00\n"
     ]
    }
   ],
   "source": [
    "c = 1\n",
    "Q = [0] * k\n",
    "N = np.zeros(k)\n",
    "cumulative_reward = 0   # just for statistics purpose\n",
    "\n",
    "#first loop overe each action is neccery to avoid division by N[i] == 0\n",
    "for i in range(k):\n",
    "    reward = lib.get_action_reward(q_real[i], standard_deviation)\n",
    "    N[i] += 1\n",
    "    Q[i] += (reward - Q[i]) / N[i]\n",
    "    cumulative_reward += reward\n",
    "    \n",
    "    \n",
    "for i in range(k, iteration_no):\n",
    "    UCB = Q + c * np.sqrt(math.log(i) / N)\n",
    "    action = lib.get_greedy_action(UCB)\n",
    "    reward = lib.get_action_reward(q_real[action], standard_deviation)\n",
    "    N[action] += 1\n",
    "    Q[action] += (reward - Q[action]) / N[action]\n",
    "    cumulative_reward += reward\n",
    "\n",
    "lib.print_stats(\n",
    "    q_real=q_real,\n",
    "    N=N,\n",
    "    iteration_no=iteration_no,\n",
    "    cumulative_reward=cumulative_reward,\n",
    "    Q=Q,\n",
    "    c=c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Bandit Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of taking best posible action in 10000 itearation: 76.82 %, additional_factor:  0.25\n",
      "\n",
      "Average reward:     8.50\n",
      "\n",
      "H[0] = -117.22\tq_real[0] =  0.00\tpolicy[0] =  0.00\tN[0] =    53\n",
      "H[1] = -117.35\tq_real[1] =  4.00\tpolicy[1] =  0.00\tN[1] =    88\n",
      "H[2] = -116.86\tq_real[2] =  2.00\tpolicy[2] =  0.00\tN[2] =    64\n",
      "H[3] = -16.39\tq_real[3] =  9.00\tpolicy[3] =  0.86\tN[3] =  7682\n",
      "H[4] = -118.10\tq_real[4] =  1.00\tpolicy[4] =  0.00\tN[4] =    55\n",
      "H[5] = -117.16\tq_real[5] =  3.00\tpolicy[5] =  0.00\tN[5] =    79\n",
      "H[6] = -18.25\tq_real[6] =  8.00\tpolicy[6] =  0.14\tN[6] =  1534\n",
      "H[7] = -116.52\tq_real[7] =  6.00\tpolicy[7] =  0.00\tN[7] =   142\n",
      "H[8] = -116.51\tq_real[8] =  7.00\tpolicy[8] =  0.00\tN[8] =   191\n",
      "H[9] = -116.49\tq_real[9] =  5.00\tpolicy[9] =  0.00\tN[9] =   112\n"
     ]
    }
   ],
   "source": [
    "#Unstable due to numerical errors\n",
    "alpha = 0.25\n",
    "H = np.zeros(k)\n",
    "N = np.zeros(k)   # just for statistics purpose\n",
    "cumulative_reward = 0   # just for statistics purpose\n",
    "\n",
    "policy = np.full(k, 1/k)\n",
    "reward_mean = 0\n",
    "\n",
    "def get_action_gradient_bandit_algorithm(policy: np.array, probability: float):\n",
    "    for i, numerical_preference in enumerate(policy):\n",
    "        if probability < numerical_preference:\n",
    "            return i\n",
    "        else:\n",
    "            probability -= numerical_preference\n",
    "    print(probability, np.sum(policy), policy)\n",
    "    raise Exception(\"Numerical error\")\n",
    "    \n",
    "for i in range(iteration_no):\n",
    "    probability = np.random.rand()\n",
    "    action = get_action_gradient_bandit_algorithm(policy, probability)\n",
    "    reward = lib.get_action_reward(q_real[action], standard_deviation)\n",
    "    reward_mean += (reward - reward_mean)/(i + 1)\n",
    "    for j in range(k):\n",
    "        if j == action:\n",
    "            H[j] += alpha * (reward - reward_mean) * (1.0 - policy[j])\n",
    "        else:\n",
    "            H[j] += alpha * (reward - reward_mean) * policy[j]\n",
    "\n",
    "    policy = np.exp(H) / np.sum(np.exp(H))\n",
    "    N[action] += 1\n",
    "    cumulative_reward += reward\n",
    "    \n",
    "lib.print_stats(\n",
    "    q_real=q_real,\n",
    "    N=N,\n",
    "    iteration_no=iteration_no,\n",
    "    cumulative_reward=cumulative_reward,\n",
    "    policy=policy,\n",
    "    H=H,\n",
    "    alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
